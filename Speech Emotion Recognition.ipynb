{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37e29415-7568-4db3-abde-de8895d8ec5b",
   "metadata": {},
   "source": [
    "Speech Emotion Recognition with librosa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d4eeac-3784-4d06-be4c-89b8888924fa",
   "metadata": {},
   "source": [
    "#First we need to install the required liberaries for the Project\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b506e95f-4cac-4776-9720-fc4d9f974474",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: librosa in c:\\users\\akhilesh\\anaconda3\\lib\\site-packages (0.10.1)\n",
      "Requirement already satisfied: audioread>=2.1.9 in c:\\users\\akhilesh\\anaconda3\\lib\\site-packages (from librosa) (3.0.1)\n",
      "Requirement already satisfied: numpy!=1.22.0,!=1.22.1,!=1.22.2,>=1.20.3 in c:\\users\\akhilesh\\anaconda3\\lib\\site-packages (from librosa) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.2.0 in c:\\users\\akhilesh\\anaconda3\\lib\\site-packages (from librosa) (1.11.4)\n",
      "Requirement already satisfied: scikit-learn>=0.20.0 in c:\\users\\akhilesh\\anaconda3\\lib\\site-packages (from librosa) (1.2.2)\n",
      "Requirement already satisfied: joblib>=0.14 in c:\\users\\akhilesh\\anaconda3\\lib\\site-packages (from librosa) (1.2.0)\n",
      "Requirement already satisfied: decorator>=4.3.0 in c:\\users\\akhilesh\\anaconda3\\lib\\site-packages (from librosa) (5.1.1)\n",
      "Requirement already satisfied: numba>=0.51.0 in c:\\users\\akhilesh\\anaconda3\\lib\\site-packages (from librosa) (0.59.0)\n",
      "Requirement already satisfied: soundfile>=0.12.1 in c:\\users\\akhilesh\\anaconda3\\lib\\site-packages (from librosa) (0.12.1)\n",
      "Requirement already satisfied: pooch>=1.0 in c:\\users\\akhilesh\\anaconda3\\lib\\site-packages (from librosa) (1.8.1)\n",
      "Requirement already satisfied: soxr>=0.3.2 in c:\\users\\akhilesh\\anaconda3\\lib\\site-packages (from librosa) (0.3.7)\n",
      "Requirement already satisfied: typing-extensions>=4.1.1 in c:\\users\\akhilesh\\anaconda3\\lib\\site-packages (from librosa) (4.9.0)\n",
      "Requirement already satisfied: lazy-loader>=0.1 in c:\\users\\akhilesh\\anaconda3\\lib\\site-packages (from librosa) (0.3)\n",
      "Requirement already satisfied: msgpack>=1.0 in c:\\users\\akhilesh\\anaconda3\\lib\\site-packages (from librosa) (1.0.3)\n",
      "Requirement already satisfied: llvmlite<0.43,>=0.42.0dev0 in c:\\users\\akhilesh\\anaconda3\\lib\\site-packages (from numba>=0.51.0->librosa) (0.42.0)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in c:\\users\\akhilesh\\anaconda3\\lib\\site-packages (from pooch>=1.0->librosa) (3.10.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\akhilesh\\anaconda3\\lib\\site-packages (from pooch>=1.0->librosa) (23.1)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\akhilesh\\anaconda3\\lib\\site-packages (from pooch>=1.0->librosa) (2.31.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\akhilesh\\anaconda3\\lib\\site-packages (from scikit-learn>=0.20.0->librosa) (2.2.0)\n",
      "Requirement already satisfied: cffi>=1.0 in c:\\users\\akhilesh\\anaconda3\\lib\\site-packages (from soundfile>=0.12.1->librosa) (1.16.0)\n",
      "Requirement already satisfied: pycparser in c:\\users\\akhilesh\\anaconda3\\lib\\site-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.21)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\akhilesh\\anaconda3\\lib\\site-packages (from requests>=2.19.0->pooch>=1.0->librosa) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\akhilesh\\anaconda3\\lib\\site-packages (from requests>=2.19.0->pooch>=1.0->librosa) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\akhilesh\\anaconda3\\lib\\site-packages (from requests>=2.19.0->pooch>=1.0->librosa) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\akhilesh\\anaconda3\\lib\\site-packages (from requests>=2.19.0->pooch>=1.0->librosa) (2024.2.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5e086317-7a3a-441f-a0b4-af5ac1bffafb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: soundfile in c:\\users\\akhilesh\\anaconda3\\lib\\site-packages (0.12.1)\n",
      "Requirement already satisfied: cffi>=1.0 in c:\\users\\akhilesh\\anaconda3\\lib\\site-packages (from soundfile) (1.16.0)\n",
      "Requirement already satisfied: pycparser in c:\\users\\akhilesh\\anaconda3\\lib\\site-packages (from cffi>=1.0->soundfile) (2.21)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install soundfile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5105d2d4-b291-4ea0-8186-1e8a2afd013f",
   "metadata": {},
   "source": [
    "#What is Librosa?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e22862-ecf5-4887-b2c7-3ca2eaea5598",
   "metadata": {},
   "source": [
    "-> librosa is a Python library for analyzing audio and music. It has a flatter package layout, standardizes interfaces and names, backwards compatibility, modular functions, and readable code. Further, in this Python mini-project, we demonstrate how to install it (and a few other packages) with pip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b86886c9-f605-4bd1-bf8c-1cea30c9ff50",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the Required Liberaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c51137f8-c03d-44bc-bfc2-0aba00139559",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import soundfile\n",
    "import numpy as np\n",
    "import librosa\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b10d041-d838-4596-9840-d11fef592256",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFlair - Extract features (mfcc, chroma, mel) from a sound file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a24b4104-c91a-453d-a37f-b852e247bbfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_feature(file_name, mfcc, chroma, mel):\n",
    "    with soundfile.SoundFile(file_name) as sound_file:\n",
    "        X = sound_file.read(dtype=\"float32\")\n",
    "        sample_rate = sound_file.samplerate\n",
    "        if chroma:\n",
    "            stft = np.abs(librosa.stft(X))\n",
    "        result = np.array([])\n",
    "        if mfcc:\n",
    "            mfccs = np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=40).T, axis=0)\n",
    "            result = np.hstack((result, mfccs))\n",
    "        if chroma:\n",
    "            chroma = np.mean(librosa.feature.chroma_stft(S=stft, sr=sample_rate).T, axis=0)\n",
    "            result = np.hstack((result, chroma))\n",
    "        if mel:\n",
    "            mel = np.mean(librosa.feature.melspectrogram(y=X, sr=sample_rate).T, axis=0)\n",
    "            result = np.hstack((result, mel))\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb7f4f50-b1b8-4cb3-a72f-af5caed96ae9",
   "metadata": {},
   "source": [
    "\n",
    "The extract_feature function is designed to extract audio features from a sound file. It takes the file name of the sound file as input along with three boolean parameters (mfcc, chroma, mel) that determine which features to extract.\n",
    "\n",
    "1-MFCC (Mel-frequency cepstral coefficients): MFCCs are commonly used in speech and audio processing to represent the short-term power spectrum of a sound. They capture the timbral and spectral characteristics of the audio.\n",
    "\n",
    "2-Chroma Features: Chroma features represent the energy distribution of pitch classes (i.e., notes) in the audio. They are useful for tasks related to harmony and chord recognition.\n",
    "\n",
    "3-Mel Spectrogram: Mel spectrogram represents the power spectrum of the audio signal on the Mel frequency scale. It provides a compact representation of the spectral content of the audio.\n",
    "\n",
    "Inside the function:\n",
    "\n",
    "It reads the sound file using the soundfile library.\n",
    "It computes the sample rate of the audio file.\n",
    "If specified (chroma=True), it calculates the Short-Time Fourier Transform (STFT) of the audio.\n",
    "It initializes an empty array (result) to store the extracted features.\n",
    "If specified (mfcc=True), it computes the MFCCs using the librosa library and appends them to the result array.\n",
    "If specified (chroma=True), it computes chroma features using the STFT and appends them to the result array.\n",
    "If specified (mel=True), it computes the mel spectrogram features using the librosa library and appends them to the result array.\n",
    "Finally, it returns the concatenated array of extracted features.\n",
    "\n",
    "By calling this function and passing the file name along with the desired feature parameters, you can obtain a feature representation of the audio file suitable for various machine learning tasks such as speech emotion recognition, audio classification, and more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "82d036c4-0beb-41a5-acdc-42409a19dc76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFlair - Emotions in the RAVDESS dataset\n",
    "emotions = {\n",
    "    '01': 'neutral',\n",
    "    '02': 'calm',\n",
    "    '03': 'happy',\n",
    "    '04': 'sad',\n",
    "    '05': 'angry',\n",
    "    '06': 'fearful',\n",
    "    '07': 'disgust',\n",
    "    '08': 'surprised'\n",
    "}\n",
    "# DataFlair - Emotions to observe\n",
    "observed_emotions = ['calm', 'happy', 'fearful', 'disgust']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c3ef50e-8f32-4740-b01c-a4e9a80bff48",
   "metadata": {},
   "source": [
    "Above we define a dictionary to hold numbers and the emotions available in the RAVDESS dataset, and a list to hold those we want- calm, happy, fearful, disgust."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "47ec0c1f-22e9-45ab-8beb-32df21387739",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFlair - Load the data and extract features for each sound file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9563b2d5-e288-4c88-bc9c-013370fc2575",
   "metadata": {},
   "source": [
    " Now, let’s load the data with a function load_data() – this takes in the relative size of the test set as parameter. x and y are empty lists; we’ll use the glob() function from the glob module to get all the pathnames for the sound files in our dataset. The pattern we use for this is: “C:\\\\Users\\\\AKHILESH\\\\Downloads\\\\ravdess data\\\\Actor_*\\\\*.wav”."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96471db6-1dd0-4bce-8807-18e8b2604449",
   "metadata": {},
   "source": [
    "for each such path, get the basename of the file, the emotion by splitting the name around ‘-’ and extracting the third value:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f431086-8953-4e4d-81db-667441b47228",
   "metadata": {},
   "source": [
    "Using our emotions dictionary, this number is turned into an emotion, and our function checks whether this emotion is in our list of observed_emotions; if not, it continues to the next file. It makes a call to extract_feature and stores what is returned in ‘feature’. Then, it appends the feature to x and the emotion to y. So, the list x holds the features and y holds the emotions. We call the function train_test_split with these, the test size, and a random state value, and return that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a252e4f5-bd68-45de-8486-8918185c02c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(test_size=0.2):\n",
    "    x, y = [], []\n",
    "    for file in glob.glob(\"C:\\\\Users\\\\AKHILESH\\\\Downloads\\\\speech-emotion-recognition-ravdess-data\\\\Actor_*\\\\*.wav\"):\n",
    "        file_name = os.path.basename(file)\n",
    "        emotion = emotions[file_name.split(\"-\")[2]]\n",
    "        if emotion not in observed_emotions:\n",
    "            continue\n",
    "        feature = extract_feature(file, mfcc=True, chroma=True, mel=True)\n",
    "        x.append(feature)\n",
    "        y.append(emotion)\n",
    "    return train_test_split(np.array(x), y, test_size=test_size, random_state=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ec627daf-6b95-4d1a-8e61-d46757a677aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFlair - Split the dataset\n",
    "x_train, x_test, y_train, y_test = load_data(test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1abe61db-e505-4986-a44f-fe063a21cb71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFlair - Initialize the Multi Layer Perceptron Classifier\n",
    "model = MLPClassifier(alpha=0.01, batch_size=256, epsilon=1e-08, hidden_layer_sizes=(300,),\n",
    "                      learning_rate='adaptive', max_iter=500)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d611cb7a-9f7c-421d-a2ab-49aca4783b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFlair - Train the model\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "# DataFlair - Predict for the test set\n",
    "y_pred = model.predict(x_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e0fa7aac-d0fb-4a1b-9d88-50637a003c69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 71.35%\n"
     ]
    }
   ],
   "source": [
    "# DataFlair - Calculate the accuracy of our model\n",
    "accuracy = accuracy_score(y_true=y_test, y_pred=y_pred)\n",
    "\n",
    "# DataFlair - Print the accuracy\n",
    "print(\"Accuracy: {:.2f}%\".format(accuracy * 100))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
